{\rtf1\ansi\ansicpg1252\cocoartf1561\cocoasubrtf200
{\fonttbl\f0\fswiss\fcharset0 Helvetica;}
{\colortbl;\red255\green255\blue255;\red0\green0\blue0;\red255\green255\blue255;}
{\*\expandedcolortbl;;\cssrgb\c0\c0\c0;\csgray\c100000;}
\margl1440\margr1440\vieww10800\viewh14660\viewkind0
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f0\fs24 \cf2 Get data files from: https://github.com/DistriNet/DLWF\
\
Environment info (i.e. packages needed and versions that work for me)\
I made a virtual environment with:\
python version 2.7.14\
pip version 9.0.1\
Packages to install (with versions that I have):\
\cb3 \CocoaLigature0 configobj (5.0.6)\cb1 \CocoaLigature1 \
statistics (\cb3 \CocoaLigature0 1.0.3.5)\cb1 \CocoaLigature1 \
numpy (\cb3 \CocoaLigature0 1.14.2)\cb1 \CocoaLigature1 \
keras (\cb3 \CocoaLigature0 2.1.6)\cb1 \CocoaLigature1 \
tensorflow (\cb3 \CocoaLigature0 1.7.0)\cb1 \CocoaLigature1 \
hyperas (\cb3 \CocoaLigature0 0.4)\cb1 \CocoaLigature1 \
matplotlib (\cb3 \CocoaLigature0 2.2.2)\
\
cd keras-dlwf/\cb1 \CocoaLigature1 \
\
Edit tor_create_conf.py (if needed)\
Things you might need to edit:\
config[\'91dnn\'92] should be \'91cnn\'92, \'91sdae\'92, or \'92lstm\'92 depending on which type of model you want to train\
cnn_config, sdae_config, lstm_config should be the path to the training set file (change if you move the data folder)\
config[\'91test_data\'92] should be the path to the test data (change if you want to test on different data)\
\
After editing, to generate a new tor.conf file (used in main.py), run:\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 python tor_create_conf.py\
\
Then to train the model and run the eval, run:\
python main.py\
\
\
Dataset Info:\
Closed\
tor_900w_2500tr.npz\
\pard\pardeftab560\slleading20\partightenfactor0
\cf2 Same as CW900 in paper\
Data dimensions (900*2500) by 5000\
=(number of websites*number of valid network traces each) by (size of network trace?)\
Labels\
(900*2500)\
labels[i] is the website label for data[i,]\
len(set(labels)) = 900 i.e. 900 different websites so 900 unique label choices\
\pard\pardeftab560\slleading20\pardirnatural\partightenfactor0
\cf2 \
tor_200w_2500tr.npz\
\pard\pardeftab560\slleading20\partightenfactor0
\cf2 Same as CW200 in paper\
Dataset for the top 200 websites\
Data dimensions (200*2500) by 5000\
=(number of websites in ds * number of valid network traces each) by (size of network trace?)\
Labels\
(200*2500)\
len(set(labels))=200\
\pard\pardeftab560\slleading20\pardirnatural\partightenfactor0
\cf2 \
\pard\pardeftab560\slleading20\partightenfactor0
\cf2 Open world\
2 datasets\
tor_open_400000w.npz\
Dataset for top 400,000 Alexa websites\
Single instance for each page\
\pard\pardeftab560\slleading20\pardirnatural\partightenfactor0
\cf2 \
tor_open_200w_2000tr.npz\
Additional 2000 test traces for each website of the monitored closed world CW200 (described above)\
\pard\pardeftab560\slleading20\partightenfactor0
\cf2 400,000 instances here (2000 test traces * 200 websites)\
\pard\pardeftab560\slleading20\pardirnatural\partightenfactor0
\cf2 \
\pard\pardeftab560\slleading20\partightenfactor0
\cf2 In the paper, the open world evaluation is on all 800,000 test traffic traces, half closed world (tor_open_200w_2000tr.npz) and half open world (tor_open_4000000w.npz)}